{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ir_based_qa_bert.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1g6QUC1uWYro29I-w5XB8BIKsfCx6AuRA","authorship_tag":"ABX9TyP+dq5Qfb8NuALxjrUUPb+G"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"CyyrY4pwwvQl"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Es4FOaqMKMSX"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"txaJ33cna3yK"},"source":["!pip install torch  torchvision -f https://download.pytorch.org/whl/torch_stable.html\n","!pip install transformers==2.5.1\n","!pip install wikipedia==1.4.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PzzKWqDAc7J4"},"source":["import os\n","import shutil\n","os.getcwd()\n","drive_home='/content/drive/MyDrive/ColabNotebooks/ir_based_qa_bert'\n","os.chdir(drive_home)\n","# set path with magic\n","%env DATA_DIR=./data/squad \n","\n","# download the data\n","def download_squad(version=1):\n","    if version == 1:\n","        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n","        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n","    else:\n","        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n","        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n","            \n","download_squad(version=2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"l98zDJVidvqg"},"source":["import json\n","f = open('data/squad/dev-v2.0.json','r')\n","dev = json.loads(f.read())\n","print(dev)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESKtYObMxFIh"},"source":["import os\n","import shutil\n","os.getcwd()\n","drive_home='/content/drive/MyDrive/ColabNotebooks/ir_based_qa_bert'\n","os.chdir(drive_home)\n","# download the run_squad.py training script\n","!curl -L -O https://raw.githubusercontent.com/huggingface/transformers/b90745c5901809faef3136ed09a689e7d733526c/examples/run_squad.py\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TEINvgcQyJOx"},"source":["import os\n","import shutil\n","os.getcwd()\n","drive_home='/content/drive/MyDrive/ColabNotebooks/ir_based_qa_bert'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vQs_THUgzm9Y"},"source":["!pwd\n","!ls /content/drive/MyDrive/ColabNotebooks/ir_based_qa_bert"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jlWjUUI0nMM"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3rutcJYdlwBd"},"source":["list(filter(None, 'models/bert/checkpoint-50000'.split(\"/\"))).pop()\n","'models/bert/checkpoint-50000/'.split(\"-\")[-1].split(\"/\")[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E46X1AB0m1U_"},"source":["!cd /content/drive/MyDrive/ColabNotebooks/ir_based_qa_bert/models/bert && find ./ -name 'pytorch_model.bin'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TgvbCb3BxSkC"},"source":["!drive_home='/content/drive/MyDrive/ColabNotebooks/ir_based_qa_bert' && cd $drive_home && pwd && python run_squad.py  \\\n","    --model_type bert   \\\n","    --model_name_or_path models/bert/checkpoint-50000  \\\n","    --output_dir models/bert/ \\\n","    --data_dir data/squad   \\\n","    --overwrite_output_dir \\\n","    --do_train  \\\n","    --train_file train-v2.0.json   \\\n","    --version_2_with_negative \\\n","    --do_lower_case  \\\n","    --do_eval   \\\n","    --predict_file dev-v2.0.json   \\\n","    --per_gpu_train_batch_size 2   \\\n","    --learning_rate 3e-5   \\\n","    --num_train_epochs 2.0   \\\n","    --max_seq_length 384   \\\n","    --doc_stride 128   \\\n","    --threads 10   \\\n","    --save_steps 5000 \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9RhgcicqnrUo"},"source":["import os\n","drive_home='/content/drive/MyDrive/ColabNotebooks/ir_based_qa_bert'\n","os.chdir(drive_home)\n","print(os.getcwd())\n","from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n","\n","# Load the fine-tuned model\n","tokenizer = AutoTokenizer.from_pretrained(\"./models/bert\")\n","model = AutoModelForQuestionAnswering.from_pretrained(\"./models/bert\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bbl3zg43qdfT"},"source":["import torch\n","question = \"Who ruled Macedonia\"\n","\n","context = \"\"\"Macedonia was an ancient kingdom on the periphery of Archaic and Classical Greece, \n","and later the dominant state of Hellenistic Greece. The kingdom was founded and initially ruled \n","by the Argead dynasty, followed by the Antipatrid and Antigonid dynasties. Home to the ancient \n","Macedonians, it originated on the northeastern part of the Greek peninsula. Before the 4th \n","century BC, it was a small kingdom outside of the area dominated by the city-states of Athens, \n","Sparta and Thebes, and briefly subordinate to Achaemenid Persia.\"\"\"\n","\n","\n","# 1. TOKENIZE THE INPUT\n","# note: if you don't include return_tensors='pt' you'll get a list of lists which is easier for \n","# exploration but you cannot feed that into a model. \n","inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\") \n","\n","# 2. OBTAIN MODEL SCORES\n","# the AutoModelForQuestionAnswering class includes a span predictor on top of the model. \n","# the model returns answer start and end scores for each word in the text\n","answer_start_scores, answer_end_scores = model(**inputs)\n","answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score\n","answer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score\n","\n","# 3. GET THE ANSWER SPAN\n","# once we have the most likely start and end tokens, we grab all the tokens between them\n","# and convert tokens back to words!\n","tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9YpoBveXq61e"},"source":["import wikipedia as wiki\n","import pprint as pp\n","\n","question = 'What is the wingspan of an albatross?'\n","\n","results = wiki.search(question)\n","print(\"Wikipedia search results for our question:\\n\")\n","pp.pprint(results)\n","\n","page = wiki.page(results[0])\n","text = page.content\n","print(f\"\\nThe {results[0]} Wikipedia article contains {len(text)} characters.\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-vdF3IM1rNof"},"source":["inputs = tokenizer.encode_plus(question, text, return_tensors='pt')\n","print(f\"This translates into {len(inputs['input_ids'][0])} tokens.\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dSAdCqqvsLw1"},"source":["# time to chunk!\n","from collections import OrderedDict\n","\n","# identify question tokens (token_type_ids = 0)\n","qmask = inputs['token_type_ids'].lt(1)\n","qt = torch.masked_select(inputs['input_ids'], qmask)\n","print(f\"The question consists of {qt.size()[0]} tokens.\")\n","\n","chunk_size = model.config.max_position_embeddings - qt.size()[0] - 1 # the \"-1\" accounts for\n","# having to add a [SEP] token to the end of each chunk\n","print(f\"Each chunk will contain {chunk_size - 2} tokens of the Wikipedia article.\")\n","\n","# create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input\n","chunked_input = OrderedDict()\n","for k,v in inputs.items():\n","    q = torch.masked_select(v, qmask)\n","    c = torch.masked_select(v, ~qmask)\n","    chunks = torch.split(c, chunk_size)\n","\n","    for i, chunk in enumerate(chunks):\n","        if i not in chunked_input:\n","            chunked_input[i] = {}\n","\n","        thing = torch.cat((q, chunk))\n","        if i != len(chunks)-1:\n","            if k == 'input_ids':\n","                thing = torch.cat((thing, torch.tensor([102])))\n","            else:\n","                thing = torch.cat((thing, torch.tensor([1])))\n","\n","        chunked_input[i][k] = torch.unsqueeze(thing, dim=0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bG3fGp4HsdQe"},"source":["for i in range(len(chunked_input.keys())):\n","    print(f\"Number of tokens in chunk {i}: {len(chunked_input[i]['input_ids'].tolist()[0])}\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TREApMc-sr0P"},"source":["def convert_ids_to_string(tokenizer, input_ids):\n","    return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids))\n","\n","answer = ''\n","\n","# now we iterate over our chunks, looking for the best answer from each chunk\n","for _, chunk in chunked_input.items():\n","    answer_start_scores, answer_end_scores = model(**chunk)\n","\n","    answer_start = torch.argmax(answer_start_scores)\n","    answer_end = torch.argmax(answer_end_scores) + 1\n","\n","    ans = convert_ids_to_string(tokenizer, chunk['input_ids'][0][answer_start:answer_end])\n","    \n","    # if the ans == [CLS] then the model did not find a real answer in this chunk\n","    if ans != '[CLS]':\n","        answer += ans + \" / \"\n","        \n","print(answer)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r4Ep9WiBtadT"},"source":["from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n","\n","\n","class DocumentReader:\n","    def __init__(self, pretrained_model_name_or_path='bert-large-uncased'):\n","        self.READER_PATH = pretrained_model_name_or_path\n","        self.tokenizer = AutoTokenizer.from_pretrained(self.READER_PATH)\n","        self.model = AutoModelForQuestionAnswering.from_pretrained(self.READER_PATH)\n","        self.max_len = self.model.config.max_position_embeddings\n","        self.chunked = False\n","\n","    def tokenize(self, question, text):\n","        self.inputs = self.tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n","        self.input_ids = self.inputs[\"input_ids\"].tolist()[0]\n","\n","        if len(self.input_ids) > self.max_len:\n","            self.inputs = self.chunkify()\n","            self.chunked = True\n","\n","    def chunkify(self):\n","        \"\"\" \n","        Break up a long article into chunks that fit within the max token\n","        requirement for that Transformer model. \n","\n","        Calls to BERT / RoBERTa / ALBERT require the following format:\n","        [CLS] question tokens [SEP] context tokens [SEP].\n","        \"\"\"\n","\n","        # create question mask based on token_type_ids\n","        # value is 0 for question tokens, 1 for context tokens\n","        qmask = self.inputs['token_type_ids'].lt(1)\n","        qt = torch.masked_select(self.inputs['input_ids'], qmask)\n","        chunk_size = self.max_len - qt.size()[0] - 1 # the \"-1\" accounts for\n","        # having to add an ending [SEP] token to the end\n","\n","        # create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input\n","        chunked_input = OrderedDict()\n","        for k,v in self.inputs.items():\n","            q = torch.masked_select(v, qmask)\n","            c = torch.masked_select(v, ~qmask)\n","            chunks = torch.split(c, chunk_size)\n","            \n","            for i, chunk in enumerate(chunks):\n","                if i not in chunked_input:\n","                    chunked_input[i] = {}\n","\n","                thing = torch.cat((q, chunk))\n","                if i != len(chunks)-1:\n","                    if k == 'input_ids':\n","                        thing = torch.cat((thing, torch.tensor([102])))\n","                    else:\n","                        thing = torch.cat((thing, torch.tensor([1])))\n","\n","                chunked_input[i][k] = torch.unsqueeze(thing, dim=0)\n","        return chunked_input\n","\n","    def get_answer(self):\n","        if self.chunked:\n","            answer = ''\n","            for k, chunk in self.inputs.items():\n","                answer_start_scores, answer_end_scores = self.model(**chunk)\n","\n","                answer_start = torch.argmax(answer_start_scores)\n","                answer_end = torch.argmax(answer_end_scores) + 1\n","\n","                ans = self.convert_ids_to_string(chunk['input_ids'][0][answer_start:answer_end])\n","                if ans != '[CLS]':\n","                    answer += ans + \" / \"\n","            return answer\n","        else:\n","            answer_start_scores, answer_end_scores = self.model(**self.inputs)\n","\n","            answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score\n","            answer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score\n","        \n","            return self.convert_ids_to_string(self.inputs['input_ids'][0][\n","                                              answer_start:answer_end])\n","\n","    def convert_ids_to_string(self, input_ids):\n","        return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MY6GTE0ftn9D"},"source":["questions = [\n","    'Why is the sky blue?',\n","    'How many sides does a pentagon have?'\n","]\n","\n","reader = DocumentReader(\"deepset/bert-base-cased-squad2\") \n","\n","# if you trained your own model using the training cell earlier, you can access it with this:\n","#reader = DocumentReader(\"./models/bert/bbu_squad2\")\n","\n","for question in questions:\n","    print(f\"Question: {question}\")\n","    results = wiki.search(question)\n","\n","    page = wiki.page(results[0])\n","    print(f\"Top wiki result: {page}\")\n","\n","    text = page.content\n","\n","    reader.tokenize(question, text)\n","    print(f\"Answer: {reader.get_answer()}\")\n","    print()\n"],"execution_count":null,"outputs":[]}]}